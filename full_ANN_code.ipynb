{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "full ANN code.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9npj57-UzSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v-3mwegUzSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG5xVjysUzSH",
        "colab_type": "code",
        "colab": {},
        "outputId": "0ef38973-c29f-4d56-d9f2-dea3279aa283"
      },
      "source": [
        "data=pd.read_csv('C:\\\\Users\\\\kc\\\\Downloads\\\\data1.csv',header=None)\n",
        "#print(data)\n",
        "label=pd.read_csv('C:\\\\Users\\\\kc\\\\Downloads\\\\label1.csv',header=None)\n",
        "#print(label)\n",
        "#Dimension of the data array.\n",
        "print(data.shape)\n",
        "#ans:Because it is a matrix of 20000*20\n",
        "#ans:Each row signifies a  datapoint or the value of each feature of every house."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVKhn1V1UzSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2. Similarly extract training labels from label1.csv to Numpy array. What is the shape/dimension of Numpy Array? Also, convert each scalar value of output into one-hot vector. As for example: 1should get converted [0,1,0,0,0], 4 should get converted to [0,0,0,0,1] and so on. What is the shape of converted Numpy array?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G0rycnjUzSP",
        "colab_type": "code",
        "colab": {},
        "outputId": "ed6e23bf-c0cd-4d3c-b2ca-a43c7289f024"
      },
      "source": [
        "#coverting data and label to array\n",
        "npdata=np.asarray(data)\n",
        "nplabel=np.asarray(label)\n",
        "#conversion of array to one-hotDash vector form\n",
        "n_values = np.max((nplabel))+1\n",
        "print(n_values)  \n",
        "oneHotDash=np.eye(n_values)[nplabel]\n",
        "oneHotDash=np.reshape(oneHotDash,(20000,5))\n",
        "print(oneHotDash)\n",
        "print(npdata.shape)\n",
        "print(nplabel.shape)\n",
        "print(oneHot.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "[[0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]]\n",
            "(20000, 20)\n",
            "(20000, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'oneHot' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-7-6597e117ee47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnplabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moneHot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'oneHot' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwtJYg4FUzST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMxWstwDUzSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "trainData,testDataDash=train_test_split(npdata,train_size=0.8,test_size=0.2,shuffle=False)#divide data in a 80:20 ratio\n",
        "trainlabel,testLabelDash=train_test_split(oneHotDash,train_size=0.8,test_size=0.2,shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kRk2qpdUzSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validateData, testData=train_test_split(testDataDash,train_size=0.5,test_size=0.5,shuffle=False)#dividing rest20% data half half in validation and testing data\n",
        "validateLabel, testLabel=train_test_split(testLabelDash,train_size=0.5,test_size=0.5,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXqKnakSUzSd",
        "colab_type": "code",
        "colab": {},
        "outputId": "3116de79-6ca0-4485-aabf-f48e35cb8c06"
      },
      "source": [
        "print(trainData.shape)\n",
        "print(validateData.shape)\n",
        "print(testData.shape)\n",
        "print(trainData.size)\n",
        "print(validateLabel.shape)\n",
        "print(testLabelDash.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16000, 20)\n",
            "(2000, 20)\n",
            "(2000, 20)\n",
            "320000\n",
            "(2000, 5)\n",
            "(4000, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8aUyJPkUzSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYr5FitdUzSj",
        "colab_type": "code",
        "colab": {},
        "outputId": "a8cbec02-cba3-496f-cc70-410fa4d02001"
      },
      "source": [
        "h=input()#number of neurons in the hidden layer\n",
        "h=int(h)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EQtl9_WUzSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCmyJqXhUzSp",
        "colab_type": "code",
        "colab": {},
        "outputId": "46fb2362-2ed4-421c-8ab3-8558f18ed4e1"
      },
      "source": [
        "std1=(1/np.sqrt(20))#standard deviation of initialiing first weights\n",
        "std2=(1/np.sqrt(h))#standard deviation of initialiing second weights\n",
        "w1=np.random.normal(0,std1,(20,h))#gaussian random assignment of weight\n",
        "w2=np.random.normal(0,std2,(h,5))#gaussian random assignment of weights \n",
        "#print(\"std1\\n\",std1,\"std2\\n\",std2)\n",
        "print(\"w1\\n\",w1)\n",
        "print(\"w2\\n\",w2)\n",
        "print(w1.shape)\n",
        "print(w2.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w1\n",
            " [[-0.24047004  0.2873567   0.03424772  0.20379815 -0.05202134 -0.19585229\n",
            "  -0.04891052 -0.12620258  0.31774368  0.17085183  0.15995675  0.13161725\n",
            "  -0.15557189 -0.3980376   0.3881237  -0.02299794 -0.39359078  0.09676945\n",
            "  -0.1685601  -0.04712996]\n",
            " [-0.21432815  0.14787484  0.20166766  0.09998479 -0.24544886  0.06249916\n",
            "  -0.30743492 -0.11602656  0.64494331  0.03310243  0.0868487  -0.12982229\n",
            "   0.34865411  0.01696914  0.09909292  0.23088986 -0.00787634  0.00771868\n",
            "   0.12214015 -0.23686298]\n",
            " [-0.21528324  0.04691262 -0.07344418 -0.17039626  0.29988912 -0.43396045\n",
            "  -0.16520983  0.20849269 -0.08518573 -0.38416799  0.37647451 -0.0164777\n",
            "   0.17407565 -0.05423136 -0.2444788  -0.27089613 -0.15977255 -0.29567861\n",
            "   0.40192944  0.05504289]\n",
            " [-0.11526222 -0.02074273  0.05481917 -0.26753793 -0.18059968 -0.01838699\n",
            "   0.02979543 -0.05551955  0.51108649 -0.08254788  0.41730867  0.0415106\n",
            "   0.2900466   0.12484195 -0.13791823 -0.40961131  0.27917377 -0.18658602\n",
            "  -0.06306851 -0.18228845]\n",
            " [-0.07975941 -0.2175489  -0.76626182 -0.12173742  0.00649702 -0.04891864\n",
            "  -0.148297    0.04612267  0.3175971  -0.02998036  0.21261099  0.21612853\n",
            "   0.08446856 -0.01393396 -0.08811245  0.09614485  0.08091831 -0.38827238\n",
            "   0.08405548 -0.01682546]\n",
            " [-0.42911234  0.22102631  0.2071632  -0.05506749  0.32785793 -0.15801414\n",
            "  -0.06517498 -0.24421043 -0.17172162 -0.12408232 -0.14262672  0.18422796\n",
            "   0.19371707  0.1518038  -0.23081101 -0.3178765  -0.04350485  0.22571986\n",
            "   0.18369131  0.40406168]\n",
            " [ 0.03504119  0.15194619 -0.34331374  0.16512945  0.077962   -0.25312458\n",
            "   0.0873449   0.10581371  0.02850531 -0.04863372  0.3090805   0.22223071\n",
            "  -0.08155896  0.13145842  0.24886247 -0.16233189  0.21704692 -0.06803388\n",
            "  -0.11130131 -0.21400207]\n",
            " [ 0.22558494  0.15191847  0.11901621  0.03592815 -0.04475858 -0.21345341\n",
            "  -0.09115129 -0.23672758 -0.20226869 -0.40315004  0.2289646  -0.08166329\n",
            "  -0.07688448 -0.29775224  0.31487963  0.26589707 -0.3806401   0.24298328\n",
            "  -0.24476477  0.23400885]\n",
            " [-0.2163456  -0.50836774  0.00275311 -0.35064614 -0.20496591 -0.17734915\n",
            "  -0.14195627 -0.0018408   0.54966379  0.08805023  0.22495949 -0.27130458\n",
            "  -0.03805883  0.44323671  0.38532224 -0.2178221  -0.27281892 -0.1293723\n",
            "   0.05112412 -0.38895534]\n",
            " [-0.07474925  0.2373359  -0.24638184 -0.10737747 -0.26579831 -0.27632838\n",
            "   0.44184613  0.07031577 -0.26138474 -0.25768735 -0.05328817 -0.15087568\n",
            "   0.01364992 -0.15246797  0.30308167  0.22180273 -0.36269946  0.04537665\n",
            "  -0.23404162  0.00115509]\n",
            " [ 0.07960717 -0.11822676 -0.03362393  0.43216223 -0.13244958  0.07210631\n",
            "   0.26698647  0.59477401  0.31800371  0.01796167  0.07749396 -0.07569569\n",
            "   0.08972811  0.04130031 -0.36897069 -0.23107281  0.02897965  0.23880271\n",
            "   0.03186749 -0.21469094]\n",
            " [ 0.1138061   0.08545959  0.20747409  0.14548936 -0.43510329  0.08115113\n",
            "  -0.16362334  0.1447513  -0.29339163  0.12473558  0.17106837 -0.28734181\n",
            "   0.142741    0.19933653  0.13483473 -0.0495918  -0.48580952 -0.16863516\n",
            "   0.01198532 -0.33506007]\n",
            " [ 0.39749395 -0.39185802 -0.06893321  0.30052879 -0.25485339  0.28097848\n",
            "  -0.52695262  0.20745928  0.24760206  0.08077077 -0.14650196  0.08925913\n",
            "  -0.16281374  0.13491118  0.0914515   0.11259944  0.13105326 -0.19908389\n",
            "  -0.02318257  0.15342218]\n",
            " [-0.16230663  0.38057027 -0.04054571  0.38970346 -0.04450693  0.095049\n",
            "   0.36517661  0.03620726 -0.11481514  0.02308266 -0.18298443  0.44744399\n",
            "   0.00989819 -0.03158881 -0.21504466 -0.05787265 -0.29635841 -0.2918488\n",
            "   0.23365337 -0.00913697]\n",
            " [-0.14718596 -0.4868871  -0.31811574  0.03703583 -0.06052306  0.15348032\n",
            "  -0.14929026  0.42436151 -0.05973377 -0.24683516 -0.52803714 -0.2242978\n",
            "  -0.07517198  0.47271978  0.03672348  0.45890576 -0.21579657 -0.0386509\n",
            "  -0.16801181  0.02532985]\n",
            " [ 0.53262753  0.00085075 -0.20839606  0.01450344  0.08179503  0.11528754\n",
            "   0.08313952  0.00713969  0.19846241 -0.00519895 -0.23098212  0.22273631\n",
            "  -0.23738355 -0.11615524  0.05892345 -0.06315566 -0.20009899 -0.02159496\n",
            "  -0.55662055 -0.27968316]\n",
            " [ 0.28784055  0.39431643  0.17459487 -0.44736885  0.01866242 -0.10364937\n",
            "  -0.00680424  0.00875327  0.19437742 -0.13357853  0.11737266 -0.26474852\n",
            "  -0.38449763 -0.12892728 -0.03059858 -0.26780377 -0.09848829  0.20205156\n",
            "  -0.06285172  0.24375241]\n",
            " [-0.11015794  0.3517412  -0.23148676 -0.15985951  0.00485928 -0.31433556\n",
            "   0.19057903  0.17435191 -0.02036813 -0.00945347 -0.10341175 -0.05793641\n",
            "   0.17948143 -0.18070838  0.01370626 -0.2493322   0.15334621  0.28934007\n",
            "   0.45023948 -0.08200779]\n",
            " [-0.27893327  0.33119068 -0.25842513 -0.2881882  -0.29954371  0.38762397\n",
            "  -0.05904992 -0.12950259  0.24779469  0.09836248  0.20911233 -0.15070219\n",
            "   0.22640593 -0.08509002 -0.05494779  0.07542788 -0.17014097  0.02361949\n",
            "  -0.20667972  0.4493117 ]\n",
            " [-0.10735834  0.27384293 -0.56553572  0.09513289 -0.25466959 -0.26125095\n",
            "   0.2177927  -0.09742725  0.05278442 -0.17920737  0.0655416   0.1737627\n",
            "  -0.16235138 -0.20405749  0.27733735 -0.16343365  0.18185752 -0.11764459\n",
            "  -0.09721113 -0.27109698]]\n",
            "w2\n",
            " [[ 0.09849267 -0.19560107  0.09935104  0.05857193  0.17151241]\n",
            " [ 0.03392708  0.06553426  0.09518811 -0.06319624  0.56068467]\n",
            " [ 0.41290292  0.48363138  0.10995392  0.27451213 -0.16896588]\n",
            " [ 0.70376089  0.58606309 -0.13963807 -0.17256713 -0.28512892]\n",
            " [-0.14313448  0.08726068 -0.30831613  0.32298334  0.22530732]\n",
            " [-0.20159378  0.4372712   0.08708861  0.16957634 -0.12321505]\n",
            " [ 0.38935981 -0.12946789 -0.18746255 -0.11551527  0.03507566]\n",
            " [ 0.0236438   0.36651063  0.45404267 -0.1925007   0.11024938]\n",
            " [ 0.02694851 -0.30999046  0.01287181 -0.44869594  0.05948768]\n",
            " [ 0.04334562 -0.18877299  0.27162052  0.15118742  0.35436727]\n",
            " [-0.32262861  0.38105449 -0.02004345  0.40573998 -0.05476491]\n",
            " [ 0.74594754  0.31844505 -0.26823215  0.10351133  0.13314987]\n",
            " [-0.0327888   0.0860628   0.46899505  0.25271812 -0.01460348]\n",
            " [ 0.35092884 -0.20936083 -0.12430985 -0.00392252  0.18034951]\n",
            " [ 0.21305951 -0.01851114  0.08304134  0.09568734  0.0704037 ]\n",
            " [ 0.01082228  0.11148026 -0.03276525  0.34981872 -0.19398173]\n",
            " [-0.28020695  0.14260602  0.09659615  0.15666545  0.03914837]\n",
            " [ 0.21844385 -0.41649451  0.13700677  0.17295419 -0.07950743]\n",
            " [-0.11132716 -0.07603022 -0.25194984 -0.17782613  0.25777635]\n",
            " [-0.37477148 -0.20534299 -0.22777592 -0.03583614  0.15429126]]\n",
            "(20, 20)\n",
            "(20, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XZki_skUzSs",
        "colab_type": "code",
        "colab": {},
        "outputId": "0b782f56-d105-4618-ec8d-8ec65539b529"
      },
      "source": [
        "b1=np.zeros(h)\n",
        "b2=np.zeros(5)\n",
        "print(b1)\n",
        "print(b2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BbJcOcPUzSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Implement feedforward function of Neural Network. At output layer and hidden layer, implement\n",
        "#Activation function (any of the 4 activation functions). Calculate the final output from the output\n",
        "#layer. What is the shape of the output ?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP012IV_UzSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def actFunc(haha,choose):\n",
        "    if choose==1:#Sigmoid\n",
        "        return (1/(1+np.exp(-haha)))\n",
        "    elif choose==2:#ReLU\n",
        "        return (np.max(0,haha))\n",
        "    elif choose==3:#tanh\n",
        "        return (np.tanh(haha))\n",
        "    elif choose==4:#SoftMax\n",
        "        temp=np.exp(haha)\n",
        "        return temp/np.sum(temp,axis=1,keepdims=True) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM8j2tg5UzS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forwardFunc(npData):\n",
        "    a0=npData\n",
        "    z1=np.dot(a0,w1)+b1\n",
        "    a1=actFunc(z1,3)\n",
        "    z2=np.dot(a1,w2)+b2\n",
        "    a2=actFunc(z2,4) \n",
        "    return a0,z1,a1,z2,a2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3RKSWTUUzS5",
        "colab_type": "text"
      },
      "source": [
        "# Backward pass\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfLdgsTNUzS6",
        "colab_type": "code",
        "colab": {},
        "outputId": "27fcd20a-8f5a-469c-bbf2-10f5a77f8b80"
      },
      "source": [
        "epochs=int(input())\n",
        "\n",
        "batch=int(input())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "1600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP2-Eu6bUzS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha=0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X41mZt4kUzS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def costFunc(y,t):\n",
        "    return -(t*(np.log(y))+(1-t)*np.log(1-y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AckCDp7uUzTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracyFunc(output,labelData,dataSize):\n",
        "    maxInRow=np.argmax(output, axis=1)\n",
        "    maxInLabels=np.argmax(labelData, axis=1)\n",
        "    huehue=np.equal(maxInRow,maxInLabels)\n",
        "    count=np.sum(huehue)\n",
        " \n",
        "    return (count/dataSize)*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SKs7ipeUzTI",
        "colab_type": "code",
        "colab": {},
        "outputId": "5040ef0e-a644-4cf2-8340-2b7f50af8b3c"
      },
      "source": [
        "print(trainData.shape)\n",
        "print(trainData[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16000, 20)\n",
            "(20,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "nxp5Noz5UzTM",
        "colab_type": "code",
        "colab": {},
        "outputId": "3d2e28b1-59c0-4c80-f1c9-639f7a290d04"
      },
      "source": [
        "for j in range(epochs):\n",
        "    for i in range(int(16000/batch)):\n",
        "        a0,z1,a1,z2,a2=forwardFunc(trainData[i*batch:(i+1)*batch,:])\n",
        "        y=a2\n",
        "        labelbatch=trainlabel[i*batch:(i+1)*batch,:]\n",
        "        #backwardPass\n",
        "        del2=y-labelbatch\n",
        "\n",
        "        #del1=np.dot(del2,w2.T)*(1-pow(actFunc(z1,3),2))\n",
        "        #dcdw2=y-labelbatch\n",
        "        del1=np.dot(del2,w2.T)*(1-pow(actFunc(z1,3),2))\n",
        "        dcdw2=np.dot(a1.T,del2)\n",
        "        dcdw1=np.dot(a0.T,del1)\n",
        "        dcdb2=np.sum(del2,axis=0)\n",
        "        dcdb1=np.sum(del1,axis=0)\n",
        "        w1=w1-alpha*dcdw1\n",
        "        w2=w2-alpha*dcdw2\n",
        "        b2=b2-alpha*dcdb2\n",
        "        b1=b1-alpha*dcdb1\n",
        "    a0,z1,a1,z2,a2=forwardFunc(trainData)\n",
        "    ytrue=a2\n",
        "    cost=np.sum(costFunc(ytrue,trainlabel))/16000\n",
        "    print (\"cost=\",end=\"\")\n",
        "    print (cost)\n",
        "    accuracy = accuracyFunc(ytrue,trainlabel,16000.0)\n",
        "    print(\"training accuracy =\",end=\"\")\n",
        "    print (accuracy)\n",
        "    #validation accuracy\n",
        "va0,vz1,va1,vz2,va2=forwardFunc(validateData)\n",
        "vOutput=va2\n",
        "accuracy = accuracyFunc(vOutput,validateLabel,2000.0)\n",
        "print (\"validation accuracy\")\n",
        "print (accuracy)\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cost=0.6877970533320957\n",
            "training accuracy =86.80624999999999\n",
            "cost=0.6740285087378052\n",
            "training accuracy =87.09375\n",
            "cost=0.6615293394545694\n",
            "training accuracy =87.23125\n",
            "cost=0.6498711692413204\n",
            "training accuracy =87.52499999999999\n",
            "cost=0.638901709796837\n",
            "training accuracy =87.7\n",
            "cost=0.6285123151980148\n",
            "training accuracy =87.9625\n",
            "cost=0.6186138390595762\n",
            "training accuracy =88.14999999999999\n",
            "cost=0.6091524437953924\n",
            "training accuracy =88.3125\n",
            "cost=0.6001297712563797\n",
            "training accuracy =88.46875\n",
            "cost=0.5916192807934236\n",
            "training accuracy =88.60625\n",
            "cost=0.58368641032939\n",
            "training accuracy =88.775\n",
            "cost=0.576323565068057\n",
            "training accuracy =88.91250000000001\n",
            "cost=0.5694767152404238\n",
            "training accuracy =89.08125\n",
            "cost=0.5630892589552478\n",
            "training accuracy =89.2625\n",
            "cost=0.557120219159253\n",
            "training accuracy =89.4375\n",
            "cost=0.5515439570945666\n",
            "training accuracy =89.60000000000001\n",
            "cost=0.5463434125208656\n",
            "training accuracy =89.71875\n",
            "cost=0.5415035151386537\n",
            "training accuracy =89.8375\n",
            "cost=0.5370072619936043\n",
            "training accuracy =89.94375\n",
            "cost=0.53283451413015\n",
            "training accuracy =90.075\n",
            "cost=0.5289625389135728\n",
            "training accuracy =90.17500000000001\n",
            "cost=0.5253672559753797\n",
            "training accuracy =90.25\n",
            "cost=0.5220244943265057\n",
            "training accuracy =90.34375\n",
            "cost=0.5189109474129485\n",
            "training accuracy =90.45625000000001\n",
            "cost=0.5160047623249453\n",
            "training accuracy =90.51875\n",
            "cost=0.5132858204884504\n",
            "training accuracy =90.5875\n",
            "cost=0.5107358092193115\n",
            "training accuracy =90.69375\n",
            "cost=0.5083381811855783\n",
            "training accuracy =90.79374999999999\n",
            "cost=0.5060780702034418\n",
            "training accuracy =90.8625\n",
            "cost=0.5039421930739825\n",
            "training accuracy =90.90625\n",
            "cost=0.5019187365814507\n",
            "training accuracy =90.95\n",
            "cost=0.49999721922765716\n",
            "training accuracy =90.9625\n",
            "cost=0.4981683285351778\n",
            "training accuracy =90.99374999999999\n",
            "cost=0.4964237536157118\n",
            "training accuracy =91.04375\n",
            "cost=0.49475604222571984\n",
            "training accuracy =91.0875\n",
            "cost=0.49315850339617556\n",
            "training accuracy =91.16250000000001\n",
            "cost=0.4916251567386696\n",
            "training accuracy =91.2\n",
            "cost=0.49015071129565435\n",
            "training accuracy =91.23125\n",
            "cost=0.4887305502130317\n",
            "training accuracy =91.225\n",
            "cost=0.487360702915116\n",
            "training accuracy =91.26875\n",
            "cost=0.4860377974875761\n",
            "training accuracy =91.29374999999999\n",
            "cost=0.484758995822834\n",
            "training accuracy =91.29374999999999\n",
            "cost=0.48352191941370537\n",
            "training accuracy =91.30625\n",
            "cost=0.482324574583609\n",
            "training accuracy =91.3375\n",
            "cost=0.4811652840118882\n",
            "training accuracy =91.35625\n",
            "cost=0.48004262827781646\n",
            "training accuracy =91.35\n",
            "cost=0.4789553980293206\n",
            "training accuracy =91.3375\n",
            "cost=0.4779025551043838\n",
            "training accuracy =91.3125\n",
            "cost=0.4768831999018431\n",
            "training accuracy =91.325\n",
            "cost=0.4758965424769434\n",
            "training accuracy =91.3375\n",
            "cost=0.47494187580854536\n",
            "training accuracy =91.31875000000001\n",
            "cost=0.47401855084054584\n",
            "training accuracy =91.36875\n",
            "cost=0.4731259537020797\n",
            "training accuracy =91.4125\n",
            "cost=0.4722634857048521\n",
            "training accuracy =91.46875\n",
            "cost=0.4714305463925071\n",
            "training accuracy =91.475\n",
            "cost=0.47062651943008793\n",
            "training accuracy =91.49374999999999\n",
            "cost=0.4698507608701155\n",
            "training accuracy =91.5\n",
            "cost=0.4691025894848334\n",
            "training accuracy =91.50625\n",
            "cost=0.4683812792695599\n",
            "training accuracy =91.5125\n",
            "cost=0.46768605462393986\n",
            "training accuracy =91.50625\n",
            "cost=0.4670160888901931\n",
            "training accuracy =91.4875\n",
            "cost=0.46637050677800834\n",
            "training accuracy =91.49374999999999\n",
            "cost=0.4657483907691368\n",
            "training accuracy =91.49374999999999\n",
            "cost=0.46514879101834206\n",
            "training accuracy =91.50625\n",
            "cost=0.4645707377507901\n",
            "training accuracy =91.5\n",
            "cost=0.4640132548570242\n",
            "training accuracy =91.525\n",
            "cost=0.463475373360004\n",
            "training accuracy =91.5125\n",
            "cost=0.4629561436312373\n",
            "training accuracy =91.525\n",
            "cost=0.4624546455707439\n",
            "training accuracy =91.5125\n",
            "cost=0.46196999633938546\n",
            "training accuracy =91.53125\n",
            "cost=0.46150135556587657\n",
            "training accuracy =91.56875\n",
            "cost=0.4610479282017483\n",
            "training accuracy =91.5875\n",
            "cost=0.4606089653541935\n",
            "training accuracy =91.6125\n",
            "cost=0.46018376349942525\n",
            "training accuracy =91.625\n",
            "cost=0.4597716624886328\n",
            "training accuracy =91.6125\n",
            "cost=0.4593720427270291\n",
            "training accuracy =91.60000000000001\n",
            "cost=0.4589843218526973\n",
            "training accuracy =91.60625\n",
            "cost=0.45860795118000985\n",
            "training accuracy =91.60625\n",
            "cost=0.45824241211182776\n",
            "training accuracy =91.625\n",
            "cost=0.4578872126715086\n",
            "training accuracy =91.64375\n",
            "cost=0.45754188426336373\n",
            "training accuracy =91.65625\n",
            "cost=0.45720597874018937\n",
            "training accuracy =91.64375\n",
            "cost=0.45687906583892224\n",
            "training accuracy =91.66875\n",
            "cost=0.456560731039027\n",
            "training accuracy =91.66250000000001\n",
            "cost=0.45625057389994433\n",
            "training accuracy =91.66875\n",
            "cost=0.4559482069387929\n",
            "training accuracy =91.64999999999999\n",
            "cost=0.45565325511013277\n",
            "training accuracy =91.66250000000001\n",
            "cost=0.4553653559364695\n",
            "training accuracy =91.65625\n",
            "cost=0.4550841603013911\n",
            "training accuracy =91.68125\n",
            "cost=0.454809333850304\n",
            "training accuracy =91.6875\n",
            "cost=0.45454055884955336\n",
            "training accuracy =91.675\n",
            "cost=0.45427753625170525\n",
            "training accuracy =91.675\n",
            "cost=0.4540199876384347\n",
            "training accuracy =91.65625\n",
            "cost=0.4537676567060567\n",
            "training accuracy =91.675\n",
            "cost=0.45352031005002813\n",
            "training accuracy =91.675\n",
            "cost=0.45327773717772624\n",
            "training accuracy =91.6875\n",
            "cost=0.45303974986258017\n",
            "training accuracy =91.70625\n",
            "cost=0.4528061810509595\n",
            "training accuracy =91.7\n",
            "cost=0.45257688348942393\n",
            "training accuracy =91.7\n",
            "cost=0.45235172809045027\n",
            "training accuracy =91.70625\n",
            "cost=0.4521306019157009\n",
            "training accuracy =91.7\n",
            "cost=0.4519134056401187\n",
            "training accuracy =91.71875\n",
            "cost=0.45170005049202056\n",
            "training accuracy =91.73125\n",
            "cost=0.45149045486611694\n",
            "training accuracy =91.74375\n",
            "cost=0.4512845409615739\n",
            "training accuracy =91.73750000000001\n",
            "cost=0.4510822318301483\n",
            "training accuracy =91.7625\n",
            "cost=0.4508834491296892\n",
            "training accuracy =91.76875\n",
            "cost=0.4506881117205941\n",
            "training accuracy =91.7625\n",
            "cost=0.4504961350833994\n",
            "training accuracy =91.78125\n",
            "cost=0.45030743141978313\n",
            "training accuracy =91.8\n",
            "cost=0.4501219102417212\n",
            "training accuracy =91.79375\n",
            "cost=0.449939479246627\n",
            "training accuracy =91.8125\n",
            "cost=0.4497600453024338\n",
            "training accuracy =91.8125\n",
            "cost=0.44958351540818603\n",
            "training accuracy =91.85624999999999\n",
            "cost=0.44940979753992233\n",
            "training accuracy =91.86875\n",
            "cost=0.44923880133101024\n",
            "training accuracy =91.875\n",
            "cost=0.44907043856746326\n",
            "training accuracy =91.88125000000001\n",
            "cost=0.4489046235015653\n",
            "training accuracy =91.89375\n",
            "cost=0.4487412730022194\n",
            "training accuracy =91.90625\n",
            "cost=0.44858030656914616\n",
            "training accuracy =91.90625\n",
            "cost=0.4484216462418526\n",
            "training accuracy =91.89375\n",
            "cost=0.44826521643448064\n",
            "training accuracy =91.9\n",
            "cost=0.44811094372537846\n",
            "training accuracy =91.9\n",
            "cost=0.44795875662643564\n",
            "training accuracy =91.9125\n",
            "cost=0.4478085853526475\n",
            "training accuracy =91.91875\n",
            "cost=0.4476603616075944\n",
            "training accuracy =91.90625\n",
            "cost=0.4475140183959477\n",
            "training accuracy =91.91875\n",
            "cost=0.44736948987003217\n",
            "training accuracy =91.91875\n",
            "cost=0.447226711214024\n",
            "training accuracy =91.93124999999999\n",
            "cost=0.4470856185666012\n",
            "training accuracy =91.95\n",
            "cost=0.44694614898075624\n",
            "training accuracy =91.9625\n",
            "cost=0.44680824041796413\n",
            "training accuracy =91.96875\n",
            "cost=0.4466718317728323\n",
            "training accuracy =91.96875\n",
            "cost=0.44653686292365696\n",
            "training accuracy =91.975\n",
            "cost=0.4464032748038192\n",
            "training accuracy =91.975\n",
            "cost=0.44627100948858683\n",
            "training accuracy =91.9875\n",
            "cost=0.44614001029156325\n",
            "training accuracy =92.0\n",
            "cost=0.44601022186467254\n",
            "training accuracy =92.0\n",
            "cost=0.4458815902951878\n",
            "training accuracy =92.00625\n",
            "cost=0.4457540631928987\n",
            "training accuracy =92.0125\n",
            "cost=0.4456275897601131\n",
            "training accuracy =92.00625\n",
            "cost=0.44550212083690377\n",
            "training accuracy =92.03125\n",
            "cost=0.44537760891391964\n",
            "training accuracy =92.025\n",
            "cost=0.4452540081053444\n",
            "training accuracy =92.0125\n",
            "cost=0.4451312740753325\n",
            "training accuracy =92.01875000000001\n",
            "cost=0.4450093639126291\n",
            "training accuracy =92.01875000000001\n",
            "cost=0.44488823595019894\n",
            "training accuracy =92.025\n",
            "cost=0.44476784952960224\n",
            "training accuracy =92.05\n",
            "cost=0.4446481647135504\n",
            "training accuracy =92.0625\n",
            "cost=0.4445291419544019\n",
            "training accuracy =92.06875\n",
            "cost=0.44441074173106654\n",
            "training accuracy =92.08125\n",
            "cost=0.4442929241714808\n",
            "training accuracy =92.09375\n",
            "cost=0.444175648681978\n",
            "training accuracy =92.10000000000001\n",
            "cost=0.4440588736079384\n",
            "training accuracy =92.10625\n",
            "cost=0.44394255595148746\n",
            "training accuracy =92.10625\n",
            "cost=0.44382665117120884\n",
            "training accuracy =92.10625\n",
            "cost=0.44371111308557676\n",
            "training accuracy =92.10000000000001\n",
            "cost=0.4435958938960401\n",
            "training accuracy =92.0875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "cost=0.44348094433770757\n",
            "training accuracy =92.08125\n",
            "cost=0.44336621395603093\n",
            "training accuracy =92.06875\n",
            "cost=0.4432516514976571\n",
            "training accuracy =92.09375\n",
            "cost=0.4431372053938302\n",
            "training accuracy =92.11875\n",
            "cost=0.4430228243065008\n",
            "training accuracy =92.125\n",
            "cost=0.44290845770168097\n",
            "training accuracy =92.13125\n",
            "cost=0.442794056412332\n",
            "training accuracy =92.13125\n",
            "cost=0.4426795731546395\n",
            "training accuracy =92.15\n",
            "cost=0.4425649629669208\n",
            "training accuracy =92.13125\n",
            "cost=0.4424501835492704\n",
            "training accuracy =92.1375\n",
            "cost=0.4423351954936725\n",
            "training accuracy =92.14375\n",
            "cost=0.44221996240774264\n",
            "training accuracy =92.15\n",
            "cost=0.4421044509494536\n",
            "training accuracy =92.13125\n",
            "cost=0.4419886308040533\n",
            "training accuracy =92.13125\n",
            "cost=0.44187247464693324\n",
            "training accuracy =92.14375\n",
            "cost=0.44175595814660346\n",
            "training accuracy =92.14375\n",
            "cost=0.441639060069544\n",
            "training accuracy =92.1375\n",
            "cost=0.44152176255300185\n",
            "training accuracy =92.13125\n",
            "cost=0.44140405161242124\n",
            "training accuracy =92.1375\n",
            "cost=0.44128591794670213\n",
            "training accuracy =92.15625\n",
            "cost=0.4411673580964683\n",
            "training accuracy =92.16250000000001\n",
            "cost=0.4410483759974445\n",
            "training accuracy =92.15625\n",
            "cost=0.440928984952357\n",
            "training accuracy =92.16875\n",
            "cost=0.4408092100200104\n",
            "training accuracy =92.175\n",
            "cost=0.44068909078925356\n",
            "training accuracy =92.19375\n",
            "cost=0.44056868446888053\n",
            "training accuracy =92.18125\n",
            "cost=0.4404480691835023\n",
            "training accuracy =92.16875\n",
            "cost=0.4403273473223549\n",
            "training accuracy =92.16875\n",
            "cost=0.4402066487460709\n",
            "training accuracy =92.1875\n",
            "cost=0.44008613361910914\n",
            "training accuracy =92.2\n",
            "cost=0.43996599460584196\n",
            "training accuracy =92.21875\n",
            "cost=0.43984645814782364\n",
            "training accuracy =92.21249999999999\n",
            "cost=0.43972778452798833\n",
            "training accuracy =92.21249999999999\n",
            "cost=0.4396102664218115\n",
            "training accuracy =92.21249999999999\n",
            "cost=0.4394942256322146\n",
            "training accuracy =92.20625\n",
            "cost=0.4393800077019339\n",
            "training accuracy =92.20625\n",
            "cost=0.43926797409643853\n",
            "training accuracy =92.21875\n",
            "cost=0.4391584916620218\n",
            "training accuracy =92.23125\n",
            "cost=0.439051919106591\n",
            "training accuracy =92.25\n",
            "cost=0.43894859035272926\n",
            "training accuracy =92.24375\n",
            "cost=0.4388487948059238\n",
            "training accuracy =92.25\n",
            "cost=0.43875275489302773\n",
            "training accuracy =92.2625\n",
            "validation accuracy\n",
            "91.10000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzFfQyLAUzTP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck4e9gn_UzTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}